---
title: "Proyecto semestral: Hito 1"
author: "Nicolás Herrera, Yesenia Marulanda, Franco Migliorelli, Samuel Sánchez, Sebastián Urbina"
date: "Octubre 2020"
output:
  html_document:
    number_sections: yes
    theme: spacelab
    toc: yes
  pdf_document:
    toc: yes
---
# Introducción

Lo que escribio yessenia(agregué mas cosas, seba)

Motivacion:

Twitter es una de las redes sociales más utilizadas para comentar, compartir o debatir temas de actualidad y tendencias. El primer trimestre de 2020 tuvo un aumento de un 23% en usuarios diarios con respecto al año 2019, siendo una de sus principales casuas la pandemia del COVID-19. Es por esto que analizar datos de twitter se hace interesante, ya que se puede obtener información en tiempo de real de sucesos que están ocurriendo, y con el avance de la minería de datos y la facilidad con la que se pueden manipular grandes volúmenes de información, nace la motivación por explorar tweets del coronavirus en un intervalo de tiempo acotado, desde la perspectiva de los sentimientos.

Dicho lo anterior, los puntos claves que se entablaron en conversaciones de equipo fueron los siguientes:

- Identificar como se relaciona el sentimiento identificado con el contexto pais (segun mayor o menor presencia del sentimiento).
- Categorizar paises segun mayor o menor presencia de sentimiento.
- Revisar indices de felicidad.
- Identificar palabras que son clave a la hora de definir el sentimiento : Mascarilla, cuarentena...
- Usar algoritmos para predecir sentimientos de forma sistematizada.
- Entrenar modelos predecitivos en base a tweets y accuracy. BD de entrenamiento y BD extra.

# Descripción de base de datos

La base de datos consta de 44955 filas con 6 atributos cada uno. Por cada fila hay un atributo que puede tener un valor nulo, "Location", y el resto de atributos siempre tiene valor distinto de nulo.

La información más importante está en la columna "OriginalTweet", porque aquí está el mensaje del usuario. Otro atributo importante es "Location", pero tiene un nivel mayor de dificultad trabajar con este atributo, ya que se debe reconocer que ciudades y comunas pertenecen a cada país para poder realizar una exploración de datos correcto. El atributo "TweetAt" es útil para observar la variación de los datos a lo largo del tiempo. El atributo "Sentiment" sirve para determinar si un mensaje expresa un sentimiento positivo, neutro o negativo en algún grado. Esta determinación puede cambiar en el proyecto dependiendo del criterio ocupado.

Para conservar la utilidad y no infringir la privacidad de los emisores de los mensajes, los valores de los atributos "UserName" y "ScreenName" fueron reemplazados por códigos.

# Exploración de datos
El objetivo de esta sección es describir la base de datos seleccionadas, mostrando estadísticas de resumen, gráficos relevantes para la descripción y un breve análisis sobre estos.

El primer paso consiste en cargar todas las librerías que se utilizarán en este trabajo, las cuales permiten realizar gráficos y trabajar con los datos de una forma más simple y eficiente.

```{r, message = F, warning=F}
library(ggplot2)
library(dplyr)
library(tidyverse)
library(tidytext)
library(RColorBrewer)
library(wordcloud)
library(wordcloud2)
```

A continuación se procede a cargar la base de datos extraída desde [insertar link de donde se obtuvo] para poder realizar el análisis respectivo.

```{r}
train  <- read.csv("data/Corona_NLP_train.csv", encoding="Latin-1")
test <- read.csv("data/Corona_NLP_test.csv", encoding="Latin-1")
df <- rbind(train,test)
```

Tal y como se mencionó anteriormente, esta base de datos contiene 6 columnas y 44955 filas de datos. Para poder tener una referencia de cómo son estos campos, a continuación se puede observar una vista previa de las primeras filas del set de datos.

```{r}
head(df)
```

Para poder trabajar con los datos es necesario transformar  algunos formatos de las variables, en particular en este caso, se procede a transformar las variables "TweetAt" a un formato de fecha, dado que corresponde al momento en donde se realizó el tweet, y la variable "OriginalTweet" que corresponde al contenido del tweet realizado.

```{r}
df$TweetAt <- as.Date(df$TweetAt, format="%d-%m-%y")
df$OriginalTweet <- as.character(df$OriginalTweet)
df$Location <- as.character(df$Location)
```

Los formatos de cada variable del set de datos son mostrados a continuación:
```{r}
str(df)
```

Además, es importante mencionar con qué periodos se estará trabajando, por lo que el resultado de la siguiente línea de código arroja la fecha mínima y máxima del set de datos, siendo el 2 de marzo de 2020 y el 14 de abril de 2020 respectivamente.

```{r}
summary(df$TweetAt)
```

Para ver cómo están distribuidos los tweets en el tiempo, a continuación se realiza y muestra un histograma con las distribuciones de las fechas de los tweets realizados y registrados en esta base de datos.

```{r}
ggplot(data=df, aes(x=TweetAt)) + geom_histogram(position="identity", bins=30) +
  labs(title = "Distribución de las fechas de tweets", x = "fecha de publicación",
       y = "número de tweets") + theme_bw()
```

Un comportamiento particular de los datos es que a finales de marzo la cantidad de tweets registrados disminuye  considerablemente llegando a ser nulo, mientras que la mayor cantidad de tweets se encuentra concentrada durante la tercera semana de marzo y la segunda semana de abril.

Un campo importante que posee esta base de datos es la columna "Sentiment", la cual representa el sentimiento asociado al tweet registrado. A continuación se muestra la cantidad de tweets por cada tipo de sentimiento ("Extremely Negative", "Extremely Postive", "Negative", "Neutral", "Positive") durante el periodo regisreado en el set de datos.
```{r}
tweets_mes_dia <- df %>% mutate(mes_dia = format(TweetAt, "%m-%d"))
tweets_mes_dia %>% group_by(Sentiment, mes_dia) %>% summarise(n = n()) %>%
  ggplot(aes(x = mes_dia, y = n, color = Sentiment)) +
  geom_line(aes(group = Sentiment)) +
  labs(title = "Número de tweets publicados", x = "fecha de publicación",
       y = "número de tweets") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, size = 6),
        legend.position = "bottom")
```

Se puede observar en este último gráfico que en general aquellos sentimientos que prevalecen son los de "Positive" y "Negative". Por otro lado, todos los tipos de sentimientos registrados tienen un comportamiento similar de crecimiento en el tiempo. El sentimiento menos registrados en el tiempo en general corresponde al de "Extremely Negative", lo cual es interesante dado el contexto de la base de datos, en donde se esperaría que hubiese una mayor cantidad de tweets asociado a un sentimiento negativo o extremademente negativo.

En el siguiente gráfico se puede observar la cantidad de tweets por cada tipo de sentimiento durante todo el periodo de observación.
```{r}
df %>% ggplot(aes(x = Sentiment)) + geom_bar(stat="count") + coord_flip() + theme_bw()
```

Se observa que tal y como se mencionó anteriormente, los sentimientos que destacan son "Positive" y "Negative", alcanzando un total aproximado de 13000 y 11000 tweets respectivamente. El sentimiento con una menor cantidad de registros es "Extremely Negative", con un aproximado de 6000 tweets. 

Si se compara  la proporción de estos tweets con el total del periodo de observación, se tiene que el porcentaje asociado a cada sentimiento son los siguientes:

```{r}
df %>% group_by(Sentiment) %>% summarise(Proporcion = n()/nrow(df)) %>% arrange(-Proporcion)
```

Es decir, el sentimiento "Positive" representa al 27,5% de los tweets del set de datos, mientras que el 13,5% de los tweets están categorizados bajo el sentimiento "Extremely Negative".

Otro punto importante a caracterizar es la locación en donde se emiten los tweets. En la siguiente tabla, se puede observar la cantidad total de tweets registrados para el top 10 de locaciones.

```{r}
top_10 <- df %>% filter(Location!="") %>% group_by(Location) %>% summarise(N=n()/nrow(df)) %>% arrange(-N)
top_10 <- top_10[1:10,]
top_10$N <- round(top_10$N,4)
  top_10 %>% ggplot(aes(x=reorder(Location,N), y =N )) + 
    geom_bar(stat="identity") + 
  coord_flip() + 
  labs(x="Pais", y = "Proporción del total de tweets", title="Top-10 locaciones con más tweets") +
  theme_bw() 
```

En esta última tabla se puede observar que un 20% de los tweets no registran alguna locación, mientras que del porcentaje restante, los lugares más comunes son de países como Estados Unidos, Reino Unido e India.

Antes de pasar a analizar los comentarios (Tweets), se limpian y formatean en un nuevo texto con sus palabras representativas separadas en un arreglo.

```{r}
limpiar_texto <- function(texto){
    # Se convierte todo el texto a minúsculas
    nuevo_texto <- tolower(texto)
    # Eliminación de páginas web (palabras que empiezan por "http." seguidas 
    # de cualquier cosa que no sea un espacio)
    nuevo_texto <- str_replace_all(nuevo_texto,"http\\S*", "")
    # Eliminación de signos de puntuación
    nuevo_texto <- str_replace_all(nuevo_texto,"[[:punct:]]", " ")
    # Eliminación de números
    nuevo_texto <- str_replace_all(nuevo_texto,"[[:digit:]]", " ")
    # Eliminación de espacios en blanco múltiples
    nuevo_texto <- str_replace_all(nuevo_texto,"[\\s]+", " ")
    # Tokenización por palabras individuales
    nuevo_texto <- str_split(nuevo_texto, " ")[[1]]
    # Eliminación de tokens con una longitud < 2
    nuevo_texto <- keep(.x = nuevo_texto, .p = function(x){str_length(x) > 1})
    return(nuevo_texto)
}
```

Se realiza una prueba de que el proceso de limpieza es realizado con exito.
```{r}
text = "Hola mi nombre es https://www.google.cl. Como. no sé xd6666 ASDA"
limpiar_texto(text)
```
Entonces se aplica el mismo procedimiento a los textos del dataset original.
```{r}
tweets <- df %>% mutate(texto_vector = map(.x = OriginalTweet, .f = limpiar_texto))
```

```{r}
tweets %>% select(texto_vector) %>% head()
```

En donde cada valor de la columna texto_vector es un vector con cada palabra  del textos
```{r}
tweets$texto_vector[1]
```
```{r}
#unnest() nos permite realizar una expansión de los vectores de palabras que creamos, esto aumenta la dimension de filas considerablemente
tweets_expand <- tweets %>% select(-OriginalTweet) %>% unnest()
tweets_expand <- tweets_expand %>% rename(word = texto_vector)
head(tweets_expand) 
```

```{r}
#Utilizaremos la librería de stopwords para obtener palabras que no aportar información
library(stopwords)
# "word" %in% vector -> true or false
lista_stopwords <- stopwords("english")
lista_stopwords <- c(lista_stopwords, "amp","can","cant")
```

```{r}
tweets_expand <- tweets_expand %>% filter(!(word %in% lista_stopwords)) 

tweets_expand %>% group_by(Sentiment, word) %>% 
  count(word) %>% 
  group_by(Sentiment) %>% 
  top_n(10,n) %>% 
  arrange(Sentiment, desc(n)) %>% 
  ggplot(aes(x=reorder(word,n),y=n,fill=Sentiment)) + 
  geom_col() + 
  labs(y = "Frecuencia", x = "Palabras mas repetidas") + 
  coord_flip()
```

Se puede observar que, como era de esperarse, las palabras mas comentadas en todas las categorias de sentimiento son las referentes directamente a la pandemia: "covid" y "coronavirus". Se destaca de igual forma la alta popularidad de las palabras "food" y "prices", posiblemente debido al parcial desabastecimiento de productos y la subida de precios producto de la cuarentena
Se representa la misma idea anterior, de ver las palabras mas populares por sentimiento, esta vez en word clouds

```{r}
pal_neg <- c("#FC9272", "#FB6A4A", "#EF3B2C", "#CB181D", "#A50F15")
pal_neu <- c("#A1D99B", "#74C476", "#41AB5D", "#238B45", "#006D2C")
pal_pos <- c("#9ECAE1", "#6BAED6", "#4292C6", "#2171B5", "#08519C")

top <- 400
```


```{r}
#Palabras mas repetidas en comentarios negativos y extremadamente negativos
neg_tweets <- tweets_expand %>% 
  filter(Sentiment == "Negative" | Sentiment == "Extremely Negative") %>%
  count(word)
neg_tweets_top <- neg_tweets%>% 
  arrange(desc(n)) %>%
  top_n(top,n) 


set.seed(1234)
wordcloud(words = neg_tweets_top$word, scale=c(5,0.7), freq = neg_tweets_top$n, min.freq = 1,max.words=100, random.order=FALSE, rot.per=0.35, colors=pal_neg)
```

```{r}
#Palabras mas repetidas en comentarios neutrales
neu_tweets <- tweets_expand %>% 
  filter(Sentiment == "Neutral") %>%
  count(word)
neu_tweets_top <- neu_tweets%>%
  arrange(desc(n)) %>%
  top_n(top,n) 
  

set.seed(1234)
wordcloud(words = neu_tweets_top$word, scale=c(5,0.7), freq = neu_tweets_top$n, min.freq = 1,max.words=100, random.order=FALSE, rot.per=0.35, colors=pal_neu, fixed.asp = TRUE)
```

```{r}
#Palabra mas repetidas en comentarios positivos y extremadamente positivos
pos_tweets <- tweets_expand %>% 
  filter(Sentiment == "Positive" | Sentiment == "Extremely Positive") %>%
  count(word)
pos_tweets_top <- pos_tweets%>% 
  arrange(desc(n)) %>%
  top_n(top,n) 


set.seed(1234)
wordcloud(words = pos_tweets_top$word, scale=c(5,0.7), freq = pos_tweets_top$n, min.freq = 1,max.words=100, random.order=FALSE, rot.per=0.35, colors=pal_pos, fixed.asp = TRUE)
```

```{r}
#Promedio en el largo de palabras por sentimiento.
require(stringr)
cat("Negative:",mean(str_length(neg_tweets$word)),"\n")
cat("Neutral:",mean(str_length(neu_tweets$word)),"\n")
cat("Positive:",mean(str_length(pos_tweets$word)),"\n")
```

```{r}
#N-grams (?)
```


