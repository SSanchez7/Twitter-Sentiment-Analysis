---
title: "Proyecto semestral: Hito 1"
author: "Nicolás Herrera, Yesenia Marulanda, Franco Migliorelli, Samuel Sánchez, Sebastián Urbina"
date: "Octubre 2020"
output:
  html_document:
    number_sections: yes
    theme: spacelab
    toc: yes
  pdf_document:
    toc: yes
---
# Introducción

Lo que escribio yessenia

Motivación

# Descripción de base de datos

Acá describir un poco de qué se trata la base de datos

# Análisis descriptivo

```{r, message = F, warning=F}
library(ggplot2)
library(dplyr)
library(tidyverse)
library(tidytext)
```

```{r}
train  <- read.csv("data/Corona_NLP_train.csv", encoding="Latin-1")
test <- read.csv("data/Corona_NLP_test.csv", encoding="Latin-1")
df <- rbind(train,test)
```

```{r}
head(df)
```

```{r}
df$TweetAt <- as.Date(df$TweetAt, format="%d-%m-%y")
df$OriginalTweet <- as.character(df$OriginalTweet)
```

```{r}
str(df)
summary(df$TweetAt)
```

```{r}
ggplot(data=df, aes(x=TweetAt)) + geom_histogram(position="identity", bins=30) + theme_bw()
```
```{r}
tweets_mes_dia <- df %>% mutate(mes_dia = format(TweetAt, "%m-%d"))
tweets_mes_dia %>% group_by(Sentiment, mes_dia) %>% summarise(n = n()) %>%
  ggplot(aes(x = mes_dia, y = n, color = Sentiment)) +
  geom_line(aes(group = Sentiment)) +
  labs(title = "Número de tweets publicados", x = "fecha de publicación",
       y = "número de tweets") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, size = 6),
        legend.position = "bottom")
```

```{r}
# library(tidyverse)
#tweets_sentiment <- df %>% group_by(Sentiment) %>% summarise(n = n())
df %>% ggplot(aes(x = Sentiment)) + geom_bar(stat="count") + coord_flip() + theme_bw()
```

```{r}
limpiar_texto <- function(texto){
    # Se convierte todo el texto a minúsculas
    nuevo_texto <- tolower(texto)
    # Eliminación de páginas web (palabras que empiezan por "http." seguidas 
    # de cualquier cosa que no sea un espacio)
    nuevo_texto <- str_replace_all(nuevo_texto,"http\\S*", "")
    # Eliminación de signos de puntuación
    nuevo_texto <- str_replace_all(nuevo_texto,"[[:punct:]]", " ")
    # Eliminación de números
    nuevo_texto <- str_replace_all(nuevo_texto,"[[:digit:]]", " ")
    # Eliminación de espacios en blanco múltiples
    nuevo_texto <- str_replace_all(nuevo_texto,"[\\s]+", " ")
    # Tokenización por palabras individuales
    nuevo_texto <- str_split(nuevo_texto, " ")[[1]]
    # Eliminación de tokens con una longitud < 2
    nuevo_texto <- keep(.x = nuevo_texto, .p = function(x){str_length(x) > 1})
    return(nuevo_texto)
}
```

```{r}
text = "Hola mi nombre es https://www.google.cl. Como. no sé xd6666 ASDA"
limpiar_texto(text)
```

```{r}
tweets <- df %>% mutate(texto_vector = map(.x = OriginalTweet, .f = limpiar_texto))
```

```{r}
tweets %>% select(texto_vector) %>% head()
```
```{r}
#Cada valor de la columna texto_vector es un vector con cada palabara del textos
tweets$texto_vector[1]
```
```{r}
#unnest() nos permite realizar una expansión de los vectores de palabras que creamos, esto aumenta la dimension de filas considerablemente
tweets_expand <- tweets %>% select(-OriginalTweet) %>% unnest()
tweets_expand <- tweets_expand %>% rename(word = texto_vector)
head(tweets_expand) 
```

```{r}
#Utilizaremos la librería de stopwords para obtener palabras que no aportar información
library(stopwords)
# "word" %in% vector -> true or false
lista_stopwords <- stopwords("english")
lista_stopwords <- c(lista_stopwords, "amp","can")
```

```{r}
tweets_expand <- tweets_expand %>% filter(!(word %in% lista_stopwords)) 

tweets_expand %>% group_by(Sentiment, word) %>% 
  count(word) %>% 
  group_by(Sentiment) %>% 
  top_n(10,n) %>% 
  arrange(Sentiment, desc(n)) %>% 
  ggplot(aes(x=reorder(word,n),y=n,fill=Sentiment)) + 
  geom_col() + 
  labs(y = "Frecuencia", x = "Ppalabras mas repetidas") + 
  coord_flip()
```
```{r}
require(RColorBrewer)
require(wordcloud)
require(wordcloud2)
top <- 400
```

```{r}
#Palabras mas repetidas en comentarios negativos y extremadamente negativos
neg_tweets <- tweets_expand %>% 
  filter(Sentiment == "Negative" | Sentiment == "Extremely Negative") %>%
  count(word)
neg_tweets_top <- neg_tweets%>% 
  arrange(desc(n)) %>%
  top_n(top,n) 

wordcloud(head(neg_tweets_top$word, 100), head(neg_tweets_top$n, 100), random.order=FALSE, colors=brewer.pal(8, "Dark2"))
wordcloud2(data=neg_tweets_top, size=1.6, color='random-dark')
```

```{r}
#Palabras mas repetidas en comentarios neutrales
neu_tweets <- tweets_expand %>% 
  filter(Sentiment == "Neutral") %>%
  count(word)
neu_tweets_top <- neu_tweets%>%
  arrange(desc(n)) %>%
  top_n(top,n) 
  

wordcloud(head(neu_tweets_top$word, 100), head(neu_tweets_top$n, 100), random.order=FALSE, colors=brewer.pal(8, "Dark2"))
wordcloud2(data=neu_tweets_top, size=1.6, color='random-dark')
```

```{r}
#Palabra mas repetidas en comentarios positivos y extremadamente positivos
pos_tweets <- tweets_expand %>% 
  filter(Sentiment == "Positive" | Sentiment == "Extremely Positive") %>%
  count(word)
pos_tweets_top <- pos_tweets%>% 
  arrange(desc(n)) %>%
  top_n(top,n) 

wordcloud(head(pos_tweets_top$word, 100), head(pos_tweets_top$n, 100), random.order=FALSE, colors=brewer.pal(8, "Dark2"))
wordcloud2(data=pos_tweets_top, size=1.6, color='random-dark')
```

```{r}
#Promedio en el largo de palabras por sentimiento.
require(stringr)
cat("Negative:",mean(str_length(neg_tweets$word)),"\n")
cat("Neutral:",mean(str_length(neu_tweets$word)),"\n")
cat("Positive:",mean(str_length(pos_tweets$word)),"\n")
```
```{r}
#Revisar Hashtags mas populares
```

```{r}
#N-grams (?)
```


