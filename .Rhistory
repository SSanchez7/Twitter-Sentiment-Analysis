install.packages("wordcloud2")
train  <- read.csv("data/Corona_NLP_train.csv", encoding="Latin-1")
test <- read.csv("data/Corona_NLP_test.csv", encoding="Latin-1")
df <- rbind(train,test)
head(df)
df$TweetAt <- as.Date(df$TweetAt, format="%d-%m-%y")
df$OriginalTweet <- as.character(df$OriginalTweet)
str(df)
summary(df$TweetAt)
ggplot(data=df, aes(x=TweetAt)) + geom_histogram(position="identity", bins=30) +
labs(title = "Distribución de las fechas de tweets", x = "fecha de publicación",
y = "número de tweets") + theme_bw()
library(ggplot2)
library(dplyr)
library(tidyverse)
library(tidytext)
train  <- read.csv("data/Corona_NLP_train.csv", encoding="Latin-1")
test <- read.csv("data/Corona_NLP_test.csv", encoding="Latin-1")
df <- rbind(train,test)
head(df)
df$TweetAt <- as.Date(df$TweetAt, format="%d-%m-%y")
df$OriginalTweet <- as.character(df$OriginalTweet)
str(df)
summary(df$TweetAt)
ggplot(data=df, aes(x=TweetAt)) + geom_histogram(position="identity", bins=30) +
labs(title = "Distribución de las fechas de tweets", x = "fecha de publicación",
y = "número de tweets") + theme_bw()
tweets_mes_dia <- df %>% mutate(mes_dia = format(TweetAt, "%m-%d"))
tweets_mes_dia %>% group_by(Sentiment, mes_dia) %>% summarise(n = n()) %>%
ggplot(aes(x = mes_dia, y = n, color = Sentiment)) +
geom_line(aes(group = Sentiment)) +
labs(title = "Número de tweets publicados", x = "fecha de publicación",
y = "número de tweets") +
theme_bw() +
theme(axis.text.x = element_text(angle = 90, size = 6),
legend.position = "bottom")
# library(tidyverse)
#tweets_sentiment <- df %>% group_by(Sentiment) %>% summarise(n = n())
df %>% ggplot(aes(x = Sentiment)) + geom_bar(stat="count") + coord_flip() + theme_bw()
df %>% group_by(Sentiment) %>% summarise(Proporcion = n()/nrow(df)) %>% arrange(-Proporcion)
top_10 <- df %>% group_by(Location) %>% summarise(N=n()/nrow(df)) %>% arrange(-N)
top_10 <- top_10[1:10,]
top_10$N <- round(top_10$N,4)
top_10
limpiar_texto <- function(texto){
# Se convierte todo el texto a minúsculas
nuevo_texto <- tolower(texto)
# Eliminación de páginas web (palabras que empiezan por "http." seguidas
# de cualquier cosa que no sea un espacio)
nuevo_texto <- str_replace_all(nuevo_texto,"http\\S*", "")
# Eliminación de signos de puntuación
nuevo_texto <- str_replace_all(nuevo_texto,"[[:punct:]]", " ")
# Eliminación de números
nuevo_texto <- str_replace_all(nuevo_texto,"[[:digit:]]", " ")
# Eliminación de espacios en blanco múltiples
nuevo_texto <- str_replace_all(nuevo_texto,"[\\s]+", " ")
# Tokenización por palabras individuales
nuevo_texto <- str_split(nuevo_texto, " ")[[1]]
# Eliminación de tokens con una longitud < 2
nuevo_texto <- keep(.x = nuevo_texto, .p = function(x){str_length(x) > 1})
return(nuevo_texto)
}
text = "Hola mi nombre es https://www.google.cl. Como. no sé xd6666 ASDA"
limpiar_texto(text)
tweets <- df %>% mutate(texto_vector = map(.x = OriginalTweet, .f = limpiar_texto))
tweets %>% select(texto_vector) %>% head()
#Cada valor de la columna texto_vector es un vector con cada palabara del textos
tweets$texto_vector[1]
#unnest() nos permite realizar una expansión de los vectores de palabras que creamos, esto aumenta la dimension de filas considerablemente
tweets_expand <- tweets %>% select(-OriginalTweet) %>% unnest()
tweets_expand <- tweets_expand %>% rename(word = texto_vector)
head(tweets_expand)
#Utilizaremos la librería de stopwords para obtener palabras que no aportar información
library(stopwords)
# "word" %in% vector -> true or false
lista_stopwords <- stopwords("english")
lista_stopwords <- c(lista_stopwords, "amp","can")
tweets_expand <- tweets_expand %>% filter(!(word %in% lista_stopwords))
tweets_expand %>% group_by(Sentiment, word) %>%
count(word) %>%
group_by(Sentiment) %>%
top_n(10,n) %>%
arrange(Sentiment, desc(n)) %>%
ggplot(aes(x=reorder(word,n),y=n,fill=Sentiment)) +
geom_col() +
labs(y = "Frecuencia", x = "Ppalabras mas repetidas") +
coord_flip()
require(RColorBrewer)
require(wordcloud)
require(wordcloud2)
top <- 400
#Palabras mas repetidas en comentarios negativos y extremadamente negativos
neg_tweets <- tweets_expand %>%
filter(Sentiment == "Negative" | Sentiment == "Extremely Negative") %>%
count(word)
neg_tweets_top <- neg_tweets%>%
arrange(desc(n)) %>%
top_n(top,n)
wordcloud(head(neg_tweets_top$word, 100), head(neg_tweets_top$n, 100), random.order=FALSE, colors=brewer.pal(8, "Dark2"))
wordcloud2(data=neg_tweets_top, size=1.6, color='random-dark')
#Palabras mas repetidas en comentarios neutrales
neu_tweets <- tweets_expand %>%
filter(Sentiment == "Neutral") %>%
count(word)
neu_tweets_top <- neu_tweets%>%
arrange(desc(n)) %>%
top_n(top,n)
wordcloud(head(neu_tweets_top$word, 100), head(neu_tweets_top$n, 100), random.order=FALSE, colors=brewer.pal(8, "Dark2"))
wordcloud2(data=neu_tweets_top, size=1.6, color='random-dark')
#Palabra mas repetidas en comentarios positivos y extremadamente positivos
pos_tweets <- tweets_expand %>%
filter(Sentiment == "Positive" | Sentiment == "Extremely Positive") %>%
count(word)
pos_tweets_top <- pos_tweets%>%
arrange(desc(n)) %>%
top_n(top,n)
wordcloud(head(pos_tweets_top$word, 100), head(pos_tweets_top$n, 100), random.order=FALSE, colors=brewer.pal(8, "Dark2"))
wordcloud2(data=pos_tweets_top, size=1.6, color='random-dark')
#Promedio en el largo de palabras por sentimiento.
require(stringr)
cat("Negative:",mean(str_length(neg_tweets$word)),"\n")
cat("Neutral:",mean(str_length(neu_tweets$word)),"\n")
cat("Positive:",mean(str_length(pos_tweets$word)),"\n")
#Revisar Hashtags mas populares
#N-grams (?)
