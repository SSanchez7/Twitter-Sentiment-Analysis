boxplot(tweets_expand$Sentiment, str_length(tweets_expand$word),xlab = "Tipo de accidente", ylab = "Cantidad", main="Cantidad de accidentes por tipo de accidente en el año 2011")
#Promedio en el largo de palabras por sentimiento.
library(stringr)
boxplot(str_length(tweets_expand$word),xlab = "Tipo de accidente", ylab = "Cantidad", main="Cantidad de accidentes por tipo de accidente en el año 2011")
cat("Negative:",mean(str_length(neg_tweets$word)),"\n")
cat("Neutral:",mean(str_length(neu_tweets$word)),"\n")
cat("Positive:",mean(str_length(pos_tweets$word)),"\n")
#Promedio en el largo de palabras por sentimiento.
library(stringr)
boxplot(x=tweets_expand$Sentiment, str_length(tweets_expand$word),xlab = "Tipo de accidente", ylab = "Cantidad", main="Cantidad de accidentes por tipo de accidente en el año 2011")
#Promedio en el largo de palabras por sentimiento.
library(stringr)
boxplot(x=tweets_expand$Sentiment, y=str_length(tweets_expand$word),xlab = "Tipo de accidente", ylab = "Cantidad", main="Cantidad de accidentes por tipo de accidente en el año 2011")
#Promedio en el largo de palabras por sentimiento.
library(stringr)
boxplot(x=tweets_expand$Sentiment,xlab = "Tipo de accidente", ylab = "Cantidad", main="Cantidad de accidentes por tipo de accidente en el año 2011")
#Promedio en el largo de palabras por sentimiento.
library(stringr)
boxplot(str_length(tweets_expand$word),xlab = "Tipo de accidente", ylab = "Cantidad", main="Cantidad de accidentes por tipo de accidente en el año 2011")
cat("Negative:",mean(str_length(neg_tweets$word)),"\n")
cat("Neutral:",mean(str_length(neu_tweets$word)),"\n")
cat("Positive:",mean(str_length(pos_tweets$word)),"\n")
#Promedio en el largo de palabras por sentimiento.
library(stringr)
plot(str_length(tweets_expand$word),xlab = "Tipo de accidente", ylab = "Cantidad", main="Cantidad de accidentes por tipo de accidente en el año 2011")
cat("Negative:",mean(str_length(neg_tweets$word)),"\n")
cat("Neutral:",mean(str_length(neu_tweets$word)),"\n")
cat("Positive:",mean(str_length(pos_tweets$word)),"\n")
#Promedio en el largo de palabras por sentimiento.
library(stringr)
plot(tweets_expand$Sentiment, str_length(tweets_expand$word),xlab = "Tipo de accidente", ylab = "Cantidad", main="Cantidad de accidentes por tipo de accidente en el año 2011")
View(tweets_expand)
View(tweets_expand)
View(tweets_expand)
#Promedio en el largo de palabras por sentimiento.
library(stringr)
filtro_tipos <- tweets_expand[str_lenght(tweets_expand$word),-2]
#Promedio en el largo de palabras por sentimiento.
library(stringr)
filtro_tipos <- tweets_expand[str_length(tweets_expand$word),-2]
plot(filtro_tipos$Sentiment, str_length(filtro_tipos$word),xlab = "Tipo de accidente", ylab = "Cantidad", main="Cantidad de accidentes por tipo de accidente en el año 2011")
#Promedio en el largo de palabras por sentimiento.
library(stringr)
filtro_tipos <- tweets_expand[str_length(tweets_expand$word),-2]
filtro_tipos
plot(filtro_tipos$Sentiment, str_length(filtro_tipos$word),xlab = "Tipo de accidente", ylab = "Cantidad", main="Cantidad de accidentes por tipo de accidente en el año 2011")
#Promedio en el largo de palabras por sentimiento.
library(stringr)
filtro_tipos <- tweets_expand
plot(filtro_tipos$Sentiment, str_length(filtro_tipos$word),xlab = "Tipo de accidente", ylab = "Cantidad", main="Cantidad de accidentes por tipo de accidente en el año 2011")
#Promedio en el largo de palabras por sentimiento.
library(stringr)
filtro_tipos <- tweets_expand
filtro_tipos$word <- str_lenght(filtro_tipos$word)
#Promedio en el largo de palabras por sentimiento.
library(stringr)
filtro_tipos <- tweets_expand
filtro_tipos$word <- str_length(filtro_tipos$word)
filtro_tipos
plot(filtro_tipos$Sentiment, str_length(filtro_tipos$word),xlab = "Tipo de accidente", ylab = "Cantidad", main="Cantidad de accidentes por tipo de accidente en el año 2011")
#Promedio en el largo de palabras por sentimiento.
library(stringr)
filtro_tipos <- tweets_expand
filtro_tipos
filtro_tipos$word <- str_length(filtro_tipos$word)
filtro_tipos
plot(filtro_tipos$Sentiment, str_length(filtro_tipos$word),xlab = "Tipo de accidente", ylab = "Cantidad", main="Cantidad de accidentes por tipo de accidente en el año 2011")
#Promedio en el largo de palabras por sentimiento.
library(stringr)
filtro_tipos <- tweets_expand
filtro_tipos$word <- str_length(filtro_tipos$word)
filtro_tipos <- filtro_tipos[,-2]
filtro_tipos
plot(filtro_tipos$Sentiment, str_length(filtro_tipos$word),xlab = "Tipo de accidente", ylab = "Cantidad", main="Cantidad de accidentes por tipo de accidente en el año 2011")
#Promedio en el largo de palabras por sentimiento.
library(stringr)
filtro_tipos <- tweets_expand
filtro_tipos$word <- str_length(filtro_tipos$word)
filtro_tipos <- filtro_tipos[,4]
filtro_tipos
plot(filtro_tipos$Sentiment, str_length(filtro_tipos$word),xlab = "Tipo de accidente", ylab = "Cantidad", main="Cantidad de accidentes por tipo de accidente en el año 2011")
#Promedio en el largo de palabras por sentimiento.
library(stringr)
filtro_tipos <- tweets_expand
filtro_tipos$word <- str_length(filtro_tipos$word)
filtro_tipos <- filtro_tipos[,2:3]
filtro_tipos
plot(filtro_tipos$Sentiment, str_length(filtro_tipos$word),xlab = "Tipo de accidente", ylab = "Cantidad", main="Cantidad de accidentes por tipo de accidente en el año 2011")
#Promedio en el largo de palabras por sentimiento.
library(stringr)
filtro_tipos <- tweets_expand
filtro_tipos$word <- str_length(filtro_tipos$word)
filtro_tipos <- filtro_tipos[,-3:]
#Promedio en el largo de palabras por sentimiento.
library(stringr)
filtro_tipos <- tweets_expand
filtro_tipos$word <- str_length(filtro_tipos$word)
filtro_tipos <- filtro_tipos[,-3:9]
#Promedio en el largo de palabras por sentimiento.
library(stringr)
filtro_tipos <- tweets_expand
filtro_tipos$word <- str_length(filtro_tipos$word)
filtro_tipos <- filtro_tipos[,-2:9]
#Promedio en el largo de palabras por sentimiento.
library(stringr)
filtro_tipos <- tweets_expand
filtro_tipos$word <- str_length(filtro_tipos$word)
filtro_tipos <- filtro_tipos[,]
filtro_tipos
plot(filtro_tipos$Sentiment, str_length(filtro_tipos$word),xlab = "Tipo de accidente", ylab = "Cantidad", main="Cantidad de accidentes por tipo de accidente en el año 2011")
#Promedio en el largo de palabras por sentimiento.
library(stringr)
filtro_tipos <- tweets_expand
filtro_tipos$word <- str_length(filtro_tipos$word)
filtro_tipos <- filtro_tipos[,4:5]
filtro_tipos
plot(filtro_tipos$Sentiment, str_length(filtro_tipos$word),xlab = "Tipo de accidente", ylab = "Cantidad", main="Cantidad de accidentes por tipo de accidente en el año 2011")
#Promedio en el largo de palabras por sentimiento.
library(stringr)
filtro_tipos <- tweets_expand
filtro_tipos$word <- str_length(filtro_tipos$word)
filtro_tipos <- filtro_tipos[,5:6]
filtro_tipos
plot(filtro_tipos$Sentiment, str_length(filtro_tipos$word),xlab = "Tipo de accidente", ylab = "Cantidad", main="Cantidad de accidentes por tipo de accidente en el año 2011")
#Promedio en el largo de palabras por sentimiento.
library(stringr)
filtro_tipos <- tweets_expand
filtro_tipos$word <- str_length(filtro_tipos$word)
filtro_tipos <- filtro_tipos[,5:6]
plot(filtro_tipos$Sentiment, filtro_tipos$word,xlab = "Tipo de accidente", ylab = "Cantidad", main="Cantidad de accidentes por tipo de accidente en el año 2011")
#Promedio en el largo de palabras por sentimiento.
library(stringr)
filtro_tipos <- tweets_expand
filtro_tipos$word <- str_length(filtro_tipos$word)
filtro_tipos <- filtro_tipos[,5:6]
boxplot(str_length(filtro_tipos$word),xlab = "Tipo de accidente", ylab = "Cantidad", main="Cantidad de accidentes por tipo de accidente en el año 2011")
cat("Negative:",mean(str_length(neg_tweets$word)),"\n")
cat("Neutral:",mean(str_length(neu_tweets$word)),"\n")
cat("Positive:",mean(str_length(pos_tweets$word)),"\n")
#Promedio en el largo de palabras por sentimiento.
library(stringr)
boxplot(str_length(neg_tweets$word),xlab = "Negative Sentiment", ylab = "Length", main="Largo palabras en comentario negativos")
boxplot(str_length(neu_tweets$word),xlab = "Neutral Sentiment", ylab = "Length", main="Largo palabras en comentario neutrales")
boxplot(str_length(pos_tweets$word),xlab = "Positive Sentiment", ylab = "Length", main="Largo palabras en comentario positivos")
cat("Negative:",mean(str_length(neg_tweets$word)),"\n")
cat("Neutral:",mean(str_length(neu_tweets$word)),"\n")
cat("Positive:",mean(str_length(pos_tweets$word)),"\n")
unlink('hito_1_cache', recursive = TRUE)
library(ggplot2)
library(dplyr)
library(tidyverse)
library(tidytext)
library(stopwords)
library(wordcloud)
install.packages(c("wordcloud", "wordcloud2"))
library(ggplot2)
library(dplyr)
library(tidyverse)
library(tidytext)
library(stopwords)
library(wordcloud)
library(wordcloud2)
train  <- read.csv("data/Corona_NLP_train.csv", encoding="Latin-1")
test <- read.csv("data/Corona_NLP_test.csv", encoding="Latin-1")
df <- rbind(train,test)
df$TweetAt <- as.Date(df$TweetAt, format="%d-%m-%y")
df$OriginalTweet <- as.character(df$OriginalTweet)
ggplot(data=df, aes(x=TweetAt)) + geom_histogram(position="identity", bins=30) +
labs(title = "Distribución de las fechas de tweets", x = "fecha de publicación",
y = "número de tweets") + theme_bw()
tweets_mes_dia <- df %>% mutate(mes_dia = format(TweetAt, "%m-%d"))
tweets_mes_dia %>% group_by(Sentiment, mes_dia) %>% summarise(n = n()) %>%
ggplot(aes(x = mes_dia, y = n, color = Sentiment)) +
geom_line(aes(group = Sentiment)) +
labs(title = "Número de tweets publicados", x = "fecha de publicación",
y = "número de tweets") +
theme_bw() +
theme(axis.text.x = element_text(angle = 90, size = 6),
legend.position = "bottom")
# library(tidyverse)
#tweets_sentiment <- df %>% group_by(Sentiment) %>% summarise(n = n())
df %>% ggplot(aes(x = Sentiment)) + geom_bar(stat="count") + coord_flip() + theme_bw()
# library(tidyverse)
#tweets_sentiment <- df %>% group_by(Sentiment) %>% summarise(n = n())
df %>% ggplot(aes(x = Sentiment)) + geom_bar(stat="count") + coord_flip() + theme_bw()
df %>% group_by(Sentiment) %>% summarise(Proporcion = n()/nrow(df)) %>% arrange(-Proporcion)
top_10 <- df %>% group_by(Location) %>% summarise(N=n()/nrow(df)) %>% arrange(-N)
top_10 <- top_10[1:10,]
top_10$N <- round(top_10$N,4)
top_10
limpiar_texto <- function(texto){
# Se convierte todo el texto a minúsculas
nuevo_texto <- tolower(texto)
# Eliminación de páginas web (palabras que empiezan por "http." seguidas
# de cualquier cosa que no sea un espacio)
nuevo_texto <- str_replace_all(nuevo_texto,"http\\S*", "")
# Eliminación de signos de puntuación
nuevo_texto <- str_replace_all(nuevo_texto,"[[:punct:]]", " ")
# Eliminación de números
nuevo_texto <- str_replace_all(nuevo_texto,"[[:digit:]]", " ")
# Eliminación de espacios en blanco múltiples
nuevo_texto <- str_replace_all(nuevo_texto,"[\\s]+", " ")
# Tokenización por palabras individuales
nuevo_texto <- str_split(nuevo_texto, " ")[[1]]
# Eliminación de tokens con una longitud < 2
nuevo_texto <- keep(.x = nuevo_texto, .p = function(x){str_length(x) > 1})
return(nuevo_texto)
}
text = "Hola mi nombre es https://www.google.cl. Como. no sé xd6666 ASDA"
limpiar_texto(text)
tweets <- df %>% mutate(texto_vector = map(.x = OriginalTweet, .f = limpiar_texto))
tweets %>% select(texto_vector) %>% head()
tweets %>% select(texto_vector) %>% head()
#Cada valor de la columna texto_vector es un vector con cada palabra del texto
tweets$texto_vector[1]
#unnest() nos permite realizar una expansión de los vectores de palabras que creamos, esto aumenta la dimension de filas considerablemente
tweets_expand <- tweets %>% select(-OriginalTweet) %>% unnest()
tweets_expand <- tweets_expand %>% rename(word = texto_vector)
head(tweets_expand)
# "word" %in% vector -> true or false
lista_stopwords <- stopwords("english")
lista_stopwords <- c(lista_stopwords, "amp","can")
tweets_expand <- tweets_expand %>% filter(!(word %in% lista_stopwords))
tweets_expand %>% group_by(Sentiment, word) %>%
count(word) %>%
group_by(Sentiment) %>%
top_n(10,n) %>%
arrange(Sentiment, desc(n)) %>%
ggplot(aes(x=reorder(word,n),y=n,fill=Sentiment)) +
geom_col() +
labs(y = "Frecuencia", x = "Palabras mas repetidas") +
coord_flip()
#Listas de colores utilizadas en las nubes de palabras
pal_neg <- c("#FC9272", "#FB6A4A", "#EF3B2C", "#CB181D", "#A50F15")
pal_neu <- c("#A1D99B", "#74C476", "#41AB5D", "#238B45", "#006D2C")
pal_pos <- c("#9ECAE1", "#6BAED6", "#4292C6", "#2171B5", "#08519C")
top <- 400
#Palabras mas repetidas en comentarios negativos y extremadamente negativos
neg_tweets <- tweets_expand %>%
filter(Sentiment == "Negative" | Sentiment == "Extremely Negative") %>%
count(word)
neg_tweets_top <- neg_tweets%>%
arrange(desc(n)) %>%
top_n(top,n)
set.seed(1234)
wordcloud(words = neg_tweets_top$word, scale=c(5,0.7), freq = neg_tweets_top$n, min.freq = 1,max.words=100, random.order=FALSE, rot.per=0.35, colors=pal_neg)
#Palabras mas repetidas en comentarios neutrales
neu_tweets <- tweets_expand %>%
filter(Sentiment == "Neutral") %>%
count(word)
neu_tweets_top <- neu_tweets%>%
arrange(desc(n)) %>%
top_n(top,n)
set.seed(1234)
wordcloud(words = neu_tweets_top$word, scale=c(5,0.7), freq = neu_tweets_top$n, min.freq = 1,max.words=100, random.order=FALSE, rot.per=0.35, colors=pal_neu, fixed.asp = TRUE)
#Palabra mas repetidas en comentarios positivos y extremadamente positivos
pos_tweets <- tweets_expand %>%
filter(Sentiment == "Positive" | Sentiment == "Extremely Positive") %>%
count(word)
pos_tweets_top <- pos_tweets%>%
arrange(desc(n)) %>%
top_n(top,n)
set.seed(1234)
wordcloud(words = pos_tweets_top$word, scale=c(5,0.7), freq = pos_tweets_top$n, min.freq = 1,max.words=100, random.order=FALSE, rot.per=0.35, colors=pal_pos, fixed.asp = TRUE)
#Promedio en el largo de palabras por sentimiento. (Intentar hacerlos todos en un unico grafico)
library(stringr)
boxplot(str_length(neg_tweets$word),xlab = "Negative Sentiment", ylab = "Length", main="Largo palabras en comentarios negativos")
boxplot(str_length(neu_tweets$word),xlab = "Neutral Sentiment", ylab = "Length", main="Largo palabras en comentarios neutrales")
boxplot(str_length(pos_tweets$word),xlab = "Positive Sentiment", ylab = "Length", main="Largo palabras en comentarios positivos")
cat("Negative:",mean(str_length(neg_tweets$word)),"\n")
cat("Neutral:",mean(str_length(neu_tweets$word)),"\n")
cat("Positive:",mean(str_length(pos_tweets$word)),"\n")
library(ggplot2)
library(dplyr)
library(tidyverse)
library(tidytext)
library(stopwords)
library(wordcloud)
library(wordcloud2)
train  <- read.csv("data/Corona_NLP_train.csv", encoding="Latin-1")
test <- read.csv("data/Corona_NLP_test.csv", encoding="Latin-1")
df <- rbind(train,test)
head(df)
df$TweetAt <- as.Date(df$TweetAt, format="%d-%m-%y")
df$OriginalTweet <- as.character(df$OriginalTweet)
str(df)
summary(df$TweetAt)
ggplot(data=df, aes(x=TweetAt)) + geom_histogram(position="identity", bins=30) +
labs(title = "Distribución de las fechas de tweets", x = "fecha de publicación",
y = "número de tweets") + theme_bw()
tweets_mes_dia <- df %>% mutate(mes_dia = format(TweetAt, "%m-%d"))
tweets_mes_dia %>% group_by(Sentiment, mes_dia) %>% summarise(n = n()) %>%
ggplot(aes(x = mes_dia, y = n, color = Sentiment)) +
geom_line(aes(group = Sentiment)) +
labs(title = "Número de tweets publicados", x = "fecha de publicación",
y = "número de tweets") +
theme_bw() +
theme(axis.text.x = element_text(angle = 90, size = 6),
legend.position = "bottom")
# library(tidyverse)
#tweets_sentiment <- df %>% group_by(Sentiment) %>% summarise(n = n())
df %>% ggplot(aes(x = Sentiment)) + geom_bar(stat="count") + coord_flip() + theme_bw()
df %>% group_by(Sentiment) %>% summarise(Proporcion = n()/nrow(df)) %>% arrange(-Proporcion)
top_10 <- df %>% group_by(Location) %>% summarise(N=n()/nrow(df)) %>% arrange(-N)
top_10 <- top_10[1:10,]
top_10$N <- round(top_10$N,4)
top_10
limpiar_texto <- function(texto){
# Se convierte todo el texto a minúsculas
nuevo_texto <- tolower(texto)
# Eliminación de páginas web (palabras que empiezan por "http." seguidas
# de cualquier cosa que no sea un espacio)
nuevo_texto <- str_replace_all(nuevo_texto,"http\\S*", "")
# Eliminación de signos de puntuación
nuevo_texto <- str_replace_all(nuevo_texto,"[[:punct:]]", " ")
# Eliminación de números
nuevo_texto <- str_replace_all(nuevo_texto,"[[:digit:]]", " ")
# Eliminación de espacios en blanco múltiples
nuevo_texto <- str_replace_all(nuevo_texto,"[\\s]+", " ")
# Tokenización por palabras individuales
nuevo_texto <- str_split(nuevo_texto, " ")[[1]]
# Eliminación de tokens con una longitud < 2
nuevo_texto <- keep(.x = nuevo_texto, .p = function(x){str_length(x) > 1})
return(nuevo_texto)
}
text = "Hola mi nombre es https://www.google.cl. Como. no sé xd6666 ASDA"
limpiar_texto(text)
tweets <- df %>% mutate(texto_vector = map(.x = OriginalTweet, .f = limpiar_texto))
tweets %>% select(texto_vector) %>% head()
#Cada valor de la columna texto_vector es un vector con cada palabra del texto
tweets$texto_vector[1]
#unnest() nos permite realizar una expansión de los vectores de palabras que creamos, esto aumenta la dimension de filas considerablemente
tweets_expand <- tweets %>% select(-OriginalTweet) %>% unnest()
tweets_expand <- tweets_expand %>% rename(word = texto_vector)
head(tweets_expand)
# "word" %in% vector -> true or false
lista_stopwords <- stopwords("english")
lista_stopwords <- c(lista_stopwords, "amp","can")
tweets_expand <- tweets_expand %>% filter(!(word %in% lista_stopwords))
tweets_expand %>% group_by(Sentiment, word) %>%
count(word) %>%
group_by(Sentiment) %>%
top_n(10,n) %>%
arrange(Sentiment, desc(n)) %>%
ggplot(aes(x=reorder(word,n),y=n,fill=Sentiment)) +
geom_col() +
labs(y = "Frecuencia", x = "Palabras mas repetidas") +
coord_flip()
#Listas de colores utilizadas en las nubes de palabras
pal_neg <- c("#FC9272", "#FB6A4A", "#EF3B2C", "#CB181D", "#A50F15")
pal_neu <- c("#A1D99B", "#74C476", "#41AB5D", "#238B45", "#006D2C")
pal_pos <- c("#9ECAE1", "#6BAED6", "#4292C6", "#2171B5", "#08519C")
top <- 400
#Palabras mas repetidas en comentarios negativos y extremadamente negativos
neg_tweets <- tweets_expand %>%
filter(Sentiment == "Negative" | Sentiment == "Extremely Negative") %>%
count(word)
neg_tweets_top <- neg_tweets%>%
arrange(desc(n)) %>%
top_n(top,n)
set.seed(1234)
wordcloud(words = neg_tweets_top$word, scale=c(5,0.7), freq = neg_tweets_top$n, min.freq = 1,max.words=100, random.order=FALSE, rot.per=0.35, colors=pal_neg)
#Palabras mas repetidas en comentarios neutrales
neu_tweets <- tweets_expand %>%
filter(Sentiment == "Neutral") %>%
count(word)
neu_tweets_top <- neu_tweets%>%
arrange(desc(n)) %>%
top_n(top,n)
set.seed(1234)
wordcloud(words = neu_tweets_top$word, scale=c(5,0.7), freq = neu_tweets_top$n, min.freq = 1,max.words=100, random.order=FALSE, rot.per=0.35, colors=pal_neu, fixed.asp = TRUE)
#Palabra mas repetidas en comentarios positivos y extremadamente positivos
pos_tweets <- tweets_expand %>%
filter(Sentiment == "Positive" | Sentiment == "Extremely Positive") %>%
count(word)
pos_tweets_top <- pos_tweets%>%
arrange(desc(n)) %>%
top_n(top,n)
set.seed(1234)
wordcloud(words = pos_tweets_top$word, scale=c(5,0.7), freq = pos_tweets_top$n, min.freq = 1,max.words=100, random.order=FALSE, rot.per=0.35, colors=pal_pos, fixed.asp = TRUE)
#Promedio en el largo de palabras por sentimiento. (Intentar hacerlos todos en un unico grafico)
library(stringr)
boxplot(str_length(neg_tweets$word),xlab = "Negative Sentiment", ylab = "Length", main="Largo palabras en comentarios negativos")
boxplot(str_length(neu_tweets$word),xlab = "Neutral Sentiment", ylab = "Length", main="Largo palabras en comentarios neutrales")
boxplot(str_length(pos_tweets$word),xlab = "Positive Sentiment", ylab = "Length", main="Largo palabras en comentarios positivos")
cat("Negative:",mean(str_length(neg_tweets$word)),"\n")
cat("Neutral:",mean(str_length(neu_tweets$word)),"\n")
cat("Positive:",mean(str_length(pos_tweets$word)),"\n")
top_10 <- df %>% group_by(Location) %>% summarise(N=n()/nrow(df)) %>% arrange(-N)
top_10 <- top_10[1:10,]
top_10$N <- round(top_10$N,4)
ggplot(top_10,aes(x=reorder(Location,N), y =N )) +
geom_bar(stat="identity") +
coord_flip() +
labs(x="Pais", y = "Proporción del total de tweets", title="Top-10 locaciones con más tweets") +
theme_bw()
View(tweets_expand)
View(tweets_expand)
View(tweets_mes_dia)
#Promedio en el largo de palabras por sentimiento. (Intentar hacerlos todos en un unico grafico)
library(stringr)
ggplot(tweets_expand, aes(x=Sentiment, y =str_length(word) ))
#boxplot(str_length(neg_tweets$word),xlab = "Negative Sentiment", ylab = "Length", main="Largo palabras en comentarios negativos")
#boxplot(str_length(neu_tweets$word),xlab = "Neutral Sentiment", ylab = "Length", main="Largo palabras en comentarios neutrales")
#boxplot(str_length(pos_tweets$word),xlab = "Positive Sentiment", ylab = "Length", main="Largo palabras en comentarios positivos")
#cat("Negative:",mean(str_length(neg_tweets$word)),"\n")
#cat("Neutral:",mean(str_length(neu_tweets$word)),"\n")
#cat("Positive:",mean(str_length(pos_tweets$word)),"\n")
#Promedio en el largo de palabras por sentimiento. (Intentar hacerlos todos en un unico grafico)
library(stringr)
ggplot(tweets_expand, aes(x=Sentiment, y =str_length(word) )) + geom_boxplot()
#boxplot(str_length(neg_tweets$word),xlab = "Negative Sentiment", ylab = "Length", main="Largo palabras en comentarios negativos")
#boxplot(str_length(neu_tweets$word),xlab = "Neutral Sentiment", ylab = "Length", main="Largo palabras en comentarios neutrales")
#boxplot(str_length(pos_tweets$word),xlab = "Positive Sentiment", ylab = "Length", main="Largo palabras en comentarios positivos")
#cat("Negative:",mean(str_length(neg_tweets$word)),"\n")
#cat("Neutral:",mean(str_length(neu_tweets$word)),"\n")
#cat("Positive:",mean(str_length(pos_tweets$word)),"\n")
tweets_mes_dia <- df %>% mutate(mes_dia = format(TweetAt, "%m-%d"))
tweets_mes_dia %>% group_by(Sentiment, mes_dia) %>% summarise(n = n()) %>%
ggplot(aes(x = mes_dia, y = n, color = Sentiment)) +
geom_line(aes(group = Sentiment)) +
labs(title = "Número de tweets publicados", x = "fecha de publicación",
y = "número de tweets") +
theme_bw() +
theme(axis.text.x = element_text(angle = 90, size = 6),
legend.position = "bottom") +
ggsave(plot=last_plot(), dpi = 300, width = 10, height = 6, filename="plot2.png")
# library(tidyverse)
#tweets_sentiment <- df %>% group_by(Sentiment) %>% summarise(n = n())
df %>% ggplot(aes(x = Sentiment)) + geom_bar(stat="count") + coord_flip() + theme_bw()+
ggsave(plot=last_plot(), dpi = 300, width = 10, height = 6, filename="plot2.png")
# library(tidyverse)
#tweets_sentiment <- df %>% group_by(Sentiment) %>% summarise(n = n())
df %>% ggplot(aes(x = Sentiment)) + geom_bar(stat="count") + coord_flip() + theme_bw()+
ggsave(plot=last_plot(), dpi = 300, width = 10, height = 6, filename="plot3.png")
tweets_mes_dia <- df %>% mutate(mes_dia = format(TweetAt, "%m-%d"))
tweets_mes_dia %>% group_by(Sentiment, mes_dia) %>% summarise(n = n()) %>%
ggplot(aes(x = mes_dia, y = n, color = Sentiment)) +
geom_line(aes(group = Sentiment)) +
labs(title = "Número de tweets publicados", x = "fecha de publicación",
y = "número de tweets") +
theme_bw() +
theme(axis.text.x = element_text(angle = 90, size = 6),
legend.position = "bottom") +
ggsave(plot=last_plot(), dpi = 300, width = 10, height = 6, filename="plot2.png")
top_10 <- df %>% group_by(Location) %>% summarise(N=n()/nrow(df)) %>% arrange(-N)
top_10 <- top_10[1:10,]
top_10$N <- round(top_10$N,4)
ggplot(top_10,aes(x=reorder(Location,N), y =N )) +
geom_bar(stat="identity") +
coord_flip() +
labs(x="Pais", y = "Proporción del total de tweets", title="Top-10 locaciones con más tweets") +
theme_bw() +
ggsave(plot=last_plot(), dpi = 300, width = 10, height = 6, filename="plot4.png")
tweets_expand <- tweets_expand %>% filter(!(word %in% lista_stopwords))
tweets_expand %>% group_by(Sentiment, word) %>%
count(word) %>%
group_by(Sentiment) %>%
top_n(10,n) %>%
arrange(Sentiment, desc(n)) %>%
ggplot(aes(x=reorder(word,n),y=n,fill=Sentiment)) +
geom_col() +
labs(y = "Frecuencia", x = "Palabras mas repetidas") +
coord_flip()+
ggsave(plot=last_plot(), dpi = 300, width = 10, height = 6, filename="plot5.png")
#Palabras mas repetidas en comentarios negativos y extremadamente negativos
neg_tweets <- tweets_expand %>%
filter(Sentiment == "Negative" | Sentiment == "Extremely Negative") %>%
count(word)
neg_tweets_top <- neg_tweets%>%
arrange(desc(n)) %>%
top_n(top,n)
set.seed(1234)
wordcloud(words = neg_tweets_top$word, scale=c(5,0.7), freq = neg_tweets_top$n, min.freq = 1,max.words=100, random.order=FALSE, rot.per=0.35, colors=pal_neg)+
ggsave(plot=last_plot(), dpi = 300, width = 10, height = 6, filename="plot6.png")
#Palabras mas repetidas en comentarios negativos y extremadamente negativos
neg_tweets <- tweets_expand %>%
filter(Sentiment == "Negative" | Sentiment == "Extremely Negative") %>%
count(word)
neg_tweets_top <- neg_tweets%>%
arrange(desc(n)) %>%
top_n(top,n)
set.seed(1234)
xd <- wordcloud(words = neg_tweets_top$word, scale=c(5,0.7), freq = neg_tweets_top$n, min.freq = 1,max.words=100, random.order=FALSE, rot.per=0.35, colors=pal_neg)
saveWidget(xd,"1.html",selfcontained = F)
install.packages("webshot")
install.packages("webshot")
#Palabras mas repetidas en comentarios negativos y extremadamente negativos
neg_tweets <- tweets_expand %>%
filter(Sentiment == "Negative" | Sentiment == "Extremely Negative") %>%
count(word)
neg_tweets_top <- neg_tweets%>%
arrange(desc(n)) %>%
top_n(top,n)
set.seed(1234)
xd <- wordcloud(words = neg_tweets_top$word, scale=c(5,0.7), freq = neg_tweets_top$n, min.freq = 1,max.words=100, random.order=FALSE, rot.per=0.35, colors=pal_neg)
saveWidget(xd,"1.html",selfcontained = F)
#Palabras mas repetidas en comentarios negativos y extremadamente negativos
neg_tweets <- tweets_expand %>%
filter(Sentiment == "Negative" | Sentiment == "Extremely Negative") %>%
count(word)
neg_tweets_top <- neg_tweets%>%
arrange(desc(n)) %>%
top_n(top,n)
set.seed(1234)
xd <- wordcloud(words = neg_tweets_top$word, scale=c(5,0.7), freq = neg_tweets_top$n, min.freq = 1,max.words=100, random.order=FALSE, rot.per=0.35, colors=pal_neg)
webshot::install_phantomjs()
saveWidget(xd,"1.html",selfcontained = F)
ggplot(data=df, aes(x=TweetAt)) + geom_histogram(position="identity", bins=30) +
labs(title = "Distribución de las fechas de tweets", x = "fecha de publicación",
y = "número de tweets") + theme_bw()+
ggsave(plot=last_plot(), dpi = 300, width = 10, height = 6, filename="plot0.png")
